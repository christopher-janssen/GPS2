---
title: "GPS22 Project: Data Import Pipeline"
author: "Christopher Janssen"
date: "`r lubridate::today()`"
format:
  html:
    self-contained: true
editor_options:
  chunk_output_type: console
editor:
  markdown:
    wrap: 72
---

# Data Import Pipeline: Research Drive → PostGIS

This notebook imports GPS data from the research drive into PostGIS for
spatial analysis and clustering.

## Package Management Setup

```{r setup}
#| message: false
#| warning: false

source(here::here("scripts/r/setup.R"))
source(here::here("scripts/r/database.R"))
```

## Overview

We'll be:

-   Importing raw GPS data points with timestamps and coordinates
-   Data quality validation and cleaning
-   Spatial geometry creation and indexing

## Database Connection

Establish connection to PostGIS database:

```{r}
#| cache: false
# Connect to database - ensure fresh connection for each render
con <- connect_gps_db()

if (is.null(con)) {
  stop("Cannot connect to database. Please ensure PostGIS container is running: docker-compose up -d")
}
```

## Data Discovery

Locate GPS data file on research drive:

```{r}
# Locate GPS data file on research drive
gps_file_path <- file.path(path_shared, "gps.csv")

if (!file.exists(gps_file_path)) {
  stop("GPS data file not found at: ", gps_file_path)
}

message("✓ Found GPS data file: ", gps_file_path)
```

## Subject Metadata Import

Load participant subid into subjects table:

```{r}
#| cache: false
# Import subject metadata first (required for foreign key constraint)
message("Importing subject metadata...")

# Read unique subject IDs from GPS data
gps_preview <- readr::read_csv(gps_file_path, 
                              col_select = "subid",
                              n_max = Inf,
                              show_col_types = FALSE)

unique_subjects <- gps_preview |> 
  distinct(subid) |> 
  pull(subid)
message("Found ", length(unique_subjects), " unique subjects")

# Clear existing subjects
dbExecute(con, "TRUNCATE TABLE subjects CASCADE")

# Create subjects dataframe
subjects_df <- tibble(subid = unique_subjects)

# Insert subjects
dbWriteTable(con, "subjects", subjects_df, 
                  append = TRUE, row.names = FALSE)

message("✓ Imported ", nrow(subjects_df), " subjects")
beepr::beep()
```

## Raw GPS Data Import

Batch import GPS points with progress tracking:

```{r}
#| eval: false
#| cache: false
#| include: false

# Import GPS data with batch processing using normalized schema
message("Starting GPS data import from: ", basename(gps_file_path))

# Read data in chunks to manage memory
chunk_size <- 10000  # Smaller chunks for large datasets
total_imported <- 0

# Clear existing data (table created in init-scripts)
dbExecute(con, "TRUNCATE TABLE raw_gps_points CASCADE")

# Read and process data in chunks
message("Reading GPS data in chunks of ", chunk_size, " rows...")

# Use readr for efficient reading with memory optimization
gps_data <- readr::read_csv(gps_file_path, 
                           col_select = c("subid", "lat", "lon", "time", "sgmnt_type"),
                           show_col_types = FALSE,
                           lazy = FALSE,
                           progress = show_progress())

# Convert time column to proper timestamp and filter invalid coordinates
gps_data <- gps_data |>
  mutate(time = as.POSIXct(time)) |>
  filter(
    !is.na(lat), !is.na(lon),
    lat >= -90, lat <= 90,
    lon >= -180, lon <= 180
  )

message("Filtered to ", nrow(gps_data), " rows with valid coordinates")

# Convert subid to subject_id using lookup
message("Converting subject IDs to normalized foreign keys...")
subject_lookup <- dbGetQuery(con, "SELECT id, subid FROM subjects")
gps_data <- gps_data |>
  left_join(subject_lookup, by = "subid") |>
  select(-subid) |>
  rename(subject_id = id)

# Insert data in chunks
n_rows <- nrow(gps_data)
n_chunks <- ceiling(n_rows / chunk_size)

for (i in 1:n_chunks) {
  start_idx <- ((i - 1) * chunk_size) + 1
  end_idx <- min(i * chunk_size, n_rows)
  
  chunk_data <- gps_data[start_idx:end_idx, ]
  
  # Insert chunk with normalized schema
  dbWriteTable(con, "raw_gps_points", chunk_data, 
                    append = TRUE, row.names = FALSE)
  
  total_imported <- total_imported + nrow(chunk_data)
  
  # Force garbage collection every 10 chunks to manage memory
  if (i %% 10 == 0) {
    gc()
    message("Imported ", total_imported, "/", n_rows, " rows (", 
            round(100 * total_imported / n_rows, 1), "%)")
  }
}

message("✓ Completed GPS data import - ", total_imported, " points")
beepr::beep()
```

## Data Quality Validation

Validate imported data for quality issues:

```{r}
# Run data quality validation
message("Running data quality validation...")

# Check invalid coordinates
invalid_coords <- dbGetQuery(con, "
  SELECT COUNT(*) as count 
  FROM raw_gps_points 
  WHERE lat IS NULL OR lon IS NULL 
     OR lat < -90 OR lat > 90 
     OR lon < -180 OR lon > 180
")$count

# Check invalid timestamps
invalid_times <- dbGetQuery(con, "
  SELECT COUNT(*) as count 
  FROM raw_gps_points 
  WHERE time IS NULL 
     OR time > CURRENT_TIMESTAMP 
     OR time < '1990-01-01'::timestamp
")$count

# Check duplicate points (same subject_id, lat, lon, time)
duplicates <- dbGetQuery(con, "
  SELECT COUNT(*) - COUNT(DISTINCT (subject_id, lat, lon, time)) as count
  FROM raw_gps_points
")$count

# Check for outliers - points with impossible speeds (>200 km/h)
outliers <- dbGetQuery(con, "
  WITH point_speeds AS (
    SELECT 
      point_id,
      subject_id,
      ST_Distance(
        geom, 
        LAG(geom) OVER (PARTITION BY subject_id ORDER BY time)
      ) / 1000.0 as distance_km,
      EXTRACT(EPOCH FROM (
        time - LAG(time) OVER (PARTITION BY subject_id ORDER BY time)
      )) / 3600.0 as time_hours
    FROM raw_gps_points
    WHERE geom IS NOT NULL
  )
  SELECT COUNT(*) as count
  FROM point_speeds 
  WHERE time_hours > 0 
    AND (distance_km / time_hours) > 200
")$count

# Log any issues found
if (invalid_coords > 0) {
  message("⚠️  Found ", invalid_coords, " points with invalid coordinates")
}
if (invalid_times > 0) {
  message("⚠️  Found ", invalid_times, " points with invalid timestamps")
}
if (duplicates > 0) {
  message("⚠️  Found ", duplicates, " duplicate GPS points")
}
if (outliers > 0) {
  message("⚠️  Found ", outliers, " points with impossible speeds (>200 km/h)")
}

total_issues <- invalid_coords + invalid_times + duplicates + outliers
if (total_issues == 0) {
  message("✓ No data quality issues detected")
} else {
  message("⚠️  Total issues found: ", total_issues)
}

# Store results for reference
validation_results <- list(
  invalid_coordinates = invalid_coords,
  invalid_timestamps = invalid_times,
  duplicate_points = duplicates,
  outliers = outliers
)

message("Data Quality Summary:")
message("- Invalid coordinates: ", validation_results$invalid_coordinates)
message("- Invalid timestamps: ", validation_results$invalid_timestamps)
message("- Duplicate points: ", validation_results$duplicate_points)
message("- Outliers detected: ", validation_results$outliers)
```

## Data Quality Cleanup

Remove duplicate GPS points to improve data quality:

```{r}
# Remove duplicate GPS points
message("Removing duplicate GPS points...")

# Much more efficient: Use window function with self-join
duplicates_removed <- dbExecute(con, "
  DELETE FROM raw_gps_points 
  WHERE point_id IN (
    SELECT point_id 
    FROM (
      SELECT point_id,
             ROW_NUMBER() OVER (
               PARTITION BY subject_id, lat, lon, time 
               ORDER BY point_id
             ) as rn
      FROM raw_gps_points
    ) ranked
    WHERE rn > 1
  )
")

message("✓ Removed ", duplicates_removed, " duplicate points")
```

## Post-Cleanup Validation

Re-validate data after cleanup to confirm improvements:

```{r}
# Re-validate after cleaning to confirm improvements
message("Re-validating after cleanup...")

# Check remaining duplicates
remaining_duplicates <- dbGetQuery(con, "
  SELECT COUNT(*) - COUNT(DISTINCT (subject_id, lat, lon, time)) as count
  FROM raw_gps_points
")$count

# Quick check of other issues
remaining_invalid_coords <- dbGetQuery(con, "
  SELECT COUNT(*) as count 
  FROM raw_gps_points 
  WHERE lat IS NULL OR lon IS NULL 
     OR lat < -90 OR lat > 90 
     OR lon < -180 OR lon > 180
")$count

message("Post-Cleanup Data Quality Summary:")
message("- Invalid coordinates: ", remaining_invalid_coords)
message("- Duplicate points: ", remaining_duplicates)

if (remaining_duplicates == 0) {
  message("✓ All duplicate points successfully removed")
} else {
  message("⚠️  ", remaining_duplicates, " duplicate points still remain")
}
```

## Import Summary

Generate summary statistics of imported data:

```{r}
# Generate import summary using status check function
check_gps_data_status(con)
```

## Database Connection Cleanup

```{r}
# Close database connection
disconnect_gps_db(con)
```

## Next Steps

Based on the import results:

1.  **If data quality issues found**: Review and clean problematic
    records
2.  **If import successful**: Proceed to
    `03-gps-processing-clustering.qmd`
3.  **If errors encountered**: Check file formats and research drive
    connectivity

### Troubleshooting Commands

```{bash eval=FALSE}
# Check research drive mount
# docker-compose exec -T postgis ls -l /research_data/

# View database tables
# docker-compose exec -T postgis psql -U postgres -d gps_analysis -c "\dt"

# Check table row counts
# docker-compose exec -T postgis psql -U postgres -d gps_analysis -c "SELECT COUNT(*) FROM raw_gps_points;"
```