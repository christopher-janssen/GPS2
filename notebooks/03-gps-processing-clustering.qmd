---
title: "GPS22 Project: GPS Processing & Clustering"
author: "Christopher Janssen"
date: "`r lubridate::today()`"
format:
  html:
    self-contained: true
editor_options:
  chunk_output_type: console
editor:
  markdown:
    wrap: 72
---

# GPS Processing & Clustering Analysis

This notebook processes cleaned GPS data and applies clustering algorithms to attempt to identify meaningful stops and locations.

## Package Management Setup

```{r setup}
source(here::here("scripts/r/setup.R"))
source(here::here("scripts/r/database.R"))
```

## Overview

We'll be:

- filtering GPS data points from clean raw table
- Enacting the movement vs. stationary point classification
- Performing clustering algorithms 
- Minor cluster validation and statistics
- Populating gps_clusters table

## Database Connection

```{r}
# Connect to database
con <- connect_gps_db()
```

## Data Status Verification

Verify current state of imported GPS data:

```{r}
# Check current GPS data status and quality
con |> check_gps_data_status()
```

## GPS Processing with PostGIS

All distance calculations, speed analysis, filtering, and movement classification are performed in PostGIS for optimal performance. This avoids loading 900K+ rows into R memory and eliminates grouped data issues.

```{r}
# Clear existing processed data (no CASCADE - don't touch raw_gps_points!)
dbExecute(con, "TRUNCATE TABLE processed_gps_points RESTART IDENTITY")

# Process all GPS data with PostGIS window functions and spatial calculations
dbExecute(con, "
  -- Step 1: Convert timezones
  WITH timezone_converted AS (
    SELECT
      rg.point_id,
      rg.subject_id,
      rg.lat, rg.lon, rg.time, rg.geom,
      (rg.time AT TIME ZONE 'UTC') AT TIME ZONE 'America/Chicago' as dttm_obs
    FROM raw_gps_points rg
  ),
  -- Step 2: Add lagged values using window functions
  with_lags AS (
    SELECT
      *,
      LAG(geom) OVER w as prev_geom,
      LAG(dttm_obs) OVER w as prev_time
    FROM timezone_converted
    WINDOW w AS (PARTITION BY subject_id ORDER BY time)
  ),
  -- Step 3: Calculate distances and durations using PostGIS geography
  with_distances AS (
    SELECT
      *,
      COALESCE(ST_Distance(prev_geom::geography, geom::geography), 0) as dist_m,
      -- Handle DST transitions: ensure duration is never negative
      GREATEST(COALESCE(EXTRACT(EPOCH FROM (dttm_obs - prev_time)) / 60, 0), 0) as duration_mins
    FROM with_lags
  ),
  -- Step 4: Calculate speeds
  with_speeds AS (
    SELECT
      *,
      dist_m / 1609.344 as dist_miles,
      CASE
        WHEN duration_mins > 0 THEN (dist_m / 1609.344) / (duration_mins / 60)
        ELSE 0
      END as speed_mph
    FROM with_distances
  ),
  -- Step 5: Apply data quality filtering (same rules as before)
  filtered AS (
    SELECT
      *,
      CASE
        -- If short distance but zero duration, likely GPS error
        WHEN dist_miles > 0.01 AND duration_mins = 0 THEN NULL
        -- If impossibly high speed (>100 mph), mark as invalid
        WHEN speed_mph > 100 THEN NULL
        -- If long duration but short distance (>30 mins, >0.31 miles), suspicious
        WHEN duration_mins > 0.5 AND dist_miles > 0.31 THEN NULL
        ELSE duration_mins
      END as duration_clean,
      CASE
        WHEN dist_miles > 0.01 AND duration_mins = 0 THEN NULL
        WHEN speed_mph > 100 THEN NULL
        WHEN duration_mins > 0.5 AND dist_miles > 0.31 THEN NULL
        -- Prevent division by zero: require at least 0.01 minutes (0.6 seconds)
        WHEN duration_mins >= 0.01 THEN (dist_miles / (duration_mins / 60))
        ELSE 0
      END as speed_clean
    FROM with_speeds
  )
  -- Step 6: Insert into processed_gps_points with movement classification
  INSERT INTO processed_gps_points (
    raw_point_id, subject_id, lat, lon, time, dttm_obs,
    dist_miles, duration_mins, speed_mph,
    movement_state, is_stationary, cluster_id
  )
  SELECT
    point_id,
    subject_id,
    lat, lon, time, dttm_obs,
    dist_miles,
    duration_clean as duration_mins,
    speed_clean as speed_mph,
    CASE WHEN speed_clean <= 4 THEN 'stationary' ELSE 'transition' END as movement_state,
    CASE WHEN speed_clean <= 4 THEN true ELSE false END as is_stationary,
    NULL as cluster_id
  FROM filtered
  WHERE duration_clean IS NOT NULL
")

# Verify processing results
processing_summary <- dbGetQuery(con, "
  SELECT
    COUNT(*) as total_processed_points,
    COUNT(CASE WHEN is_stationary THEN 1 END) as stationary_points,
    COUNT(CASE WHEN movement_state = 'transition' THEN 1 END) as transition_points,
    ROUND(100.0 * COUNT(CASE WHEN is_stationary THEN 1 END) / COUNT(*), 1) as pct_stationary
  FROM processed_gps_points
")

print("GPS Processing Summary:")
print(processing_summary)
```

## Spatial Clustering Setup

Set clustering parameters and prepare for participant-wise clustering:

```{r}
# Set clustering parameters
radius_m <- 20  # Radius in meters for spatial proximity
min_duration_min <- 30  # Minimum duration for meaningful stop within a day

# Load stationary GPS data from database for clustering
gps_data <- dbGetQuery(con, "
  SELECT
    s.subid,
    p.lat, p.lon,
    p.dttm_obs,
    p.movement_state
  FROM processed_gps_points p
  JOIN subjects s ON p.subject_id = s.id
  WHERE p.is_stationary = true
  ORDER BY s.subid, p.dttm_obs
")

# Get list of participants with stationary data
participants_with_stationary <- gps_data |>
  distinct(subid) |>
  pull(subid)

cat("Found", length(participants_with_stationary), "participants with stationary data\n")
```

## Individual Participant Clustering

Process each participant separately to identify personal location clusters:

```{r}
# Initialize list to store all participant clusters
all_clusters <- list()

# Process each participant
for (participant in participants_with_stationary) {

  # Filter for specific participant and add date
  participant_data <- gps_data |>
    filter(subid == participant) |>
    mutate(date = as.Date(dttm_obs)) |>
    arrange(dttm_obs)

  if (nrow(participant_data) == 0) {
    next  # Skip if no stationary data
  }

  cat("Processing participant:", participant, "(", nrow(participant_data), "stationary points)\n")

  # Store participant data for daily clustering
  all_clusters[[participant]] <- participant_data
}

cat("Loaded data for", length(all_clusters), "participants\n")
```


## Daily Clustering Algorithm

Cluster stationary points within each day for each participant:

```{r}
# Process daily clusters for each participant
for (participant in names(all_clusters)) {
  
  participant_data <- all_clusters[[participant]]
  
  # Process each day separately
  daily_clusters <- participant_data |>
    group_by(date) |>
    group_modify(~ {
      day_data <- .x
      day_data$daily_cluster <- 0
      cluster_id <- 1
      
      # Cluster within this day
      for (i in 1:nrow(day_data)) {
        if (day_data$daily_cluster[i] != 0) next
        
        current_point <- day_data[i, ]
        
        # Find remaining unassigned points within radius on this day
        remaining_points <- day_data[i:nrow(day_data), ] |>
          filter(daily_cluster == 0)
        
        if (nrow(remaining_points) == 0) next
        
        # Calculate Haversine distances
        distances <- distHaversine(
          p1 = c(current_point$lon, current_point$lat),
          p2 = cbind(remaining_points$lon, remaining_points$lat)
        )
        
        # Find points within radius
        nearby_indices <- which(distances <= radius_m)
        nearby_points <- remaining_points[nearby_indices, ]
        
        # Check duration within this day
        if (nrow(nearby_points) >= 2) {
          time_span_min <- as.numeric(difftime(
            max(nearby_points$dttm_obs), 
            min(nearby_points$dttm_obs), 
            units = "mins"
          ))
          
          # If duration threshold met, assign cluster
          if (time_span_min >= min_duration_min) {
            original_indices <- which(
              day_data$dttm_obs %in% nearby_points$dttm_obs &
              day_data$daily_cluster == 0
            )
            day_data$daily_cluster[original_indices] <- cluster_id
            cluster_id <- cluster_id + 1
          }
        }
      }
      
      # Return only clustered points
      day_data |> filter(daily_cluster != 0)
    }) |>
    ungroup()
  
  # Store daily clusters back
  all_clusters[[participant]] <- daily_clusters
}
```

## Cross-Day Location Aggregation

Aggregate daily clusters into final location clusters across all days:

```{r}
# Create final cluster representatives for each participant
final_clusters <- list()

for (participant in names(all_clusters)) {
  
  daily_clusters <- all_clusters[[participant]]
  
  if (nrow(daily_clusters) == 0) {
    next  # Skip if no valid clusters
  }
  
  # Create daily location summaries
  daily_locations <- daily_clusters |>
    group_by(date, daily_cluster) |>
    summarise(
      subid = first(subid),
      lat = mean(lat),
      lon = mean(lon),
      n_points = n(),
      start_time = min(dttm_obs),
      end_time = max(dttm_obs),
      duration_min = as.numeric(difftime(max(dttm_obs), min(dttm_obs), units = "mins")),
      .groups = "drop"
    )
  
  if (nrow(daily_locations) == 0) {
    next
  }
  
  # Aggregate daily locations across days based on geographic proximity
  daily_locations$final_cluster <- 0
  cluster_id <- 1
  
  for (i in 1:nrow(daily_locations)) {
    if (daily_locations$final_cluster[i] != 0) next
    
    current_location <- daily_locations[i, ]
    
    # Find all locations (across all days) within radius
    distances_all <- distHaversine(
      p1 = c(current_location$lon, current_location$lat),
      p2 = cbind(daily_locations$lon, daily_locations$lat)
    )
    
    nearby_location_indices <- which(
      distances_all <= radius_m & daily_locations$final_cluster == 0
    )
    daily_locations$final_cluster[nearby_location_indices] <- cluster_id
    cluster_id <- cluster_id + 1
  }
  
  # Create final cluster representatives
  representatives <- daily_locations |>
    filter(final_cluster != 0) |>
    group_by(final_cluster) |>
    summarise(
      subid = first(subid),
      lat = mean(lat),
      lon = mean(lon),
      n_points = sum(n_points),
      first_visit = min(start_time),
      last_visit = max(end_time),
      total_visits = n(),
      total_duration_hours = sum(duration_min) / 60,
      unique_days = n_distinct(date),
      .groups = "drop"
    ) |>
    rename(cluster_id = final_cluster)
  
  final_clusters[[participant]] <- representatives
}

length(final_clusters)
```

## Clustering Results Summary

Combine and summarize clustering results across all participants:

```{r}
# Combine all participant clusters into single dataframe
all_participant_clusters <- bind_rows(final_clusters) |>
  arrange(subid, cluster_id)

# Generate clustering summary statistics
all_participant_clusters |>
  group_by(subid) |>
  summarise(
    total_clusters = n(),
    total_visits = sum(total_visits),
    total_duration_hours = sum(total_duration_hours),
    avg_visits_per_cluster = round(mean(total_visits), 1),
    avg_duration_per_cluster = round(mean(total_duration_hours), 1),
    .groups = "drop"
  ) |> print()
```

```{r}
# Calculate overall statistics for inline reporting
avg_clusters_per_participant <- all_participant_clusters |>
  group_by(subid) |>
  summarise(clusters = n(), .groups = "drop") |>
  summarise(avg = round(mean(clusters), 1)) |>
  pull(avg)
```

## Database Population

Update database with clustering results:

```{r}
# Update processed_gps_points with cluster assignments
# Map each GPS point to its nearest final cluster centroid
cluster_assignments <- list()

for (participant in names(all_clusters)) {
  daily_clusters <- all_clusters[[participant]]

  if (nrow(daily_clusters) == 0) next

  # Get final clusters for this participant
  participant_final <- final_clusters[[participant]]

  if (is.null(participant_final) || nrow(participant_final) == 0) next

  # For each point in daily_clusters, find nearest final cluster
  point_assignments <- daily_clusters |>
    rowwise() |>
    mutate(
      # Calculate distance to each final cluster centroid
      distances = list(distHaversine(
        c(lon, lat),
        cbind(participant_final$lon, participant_final$lat)
      )),
      # Find nearest cluster
      nearest_cluster_idx = which.min(distances[[1]]),
      min_distance = min(distances[[1]]),
      # Get the actual cluster_id from gps_clusters table (will be assigned after insertion)
      # For now, use the final_cluster number - we'll remap after database insertion
      temp_cluster_id = participant_final$cluster_id[nearest_cluster_idx]
    ) |>
    ungroup() |>
    filter(min_distance <= radius_m * 2) |>  # Only assign if within 2x clustering radius
    select(subid, lat, lon, time = dttm_obs, temp_cluster_id)

  cluster_assignments[[participant]] <- point_assignments
}

# Combine all cluster assignments
if (length(cluster_assignments) > 0) {
  all_cluster_assignments <- bind_rows(cluster_assignments)

  cat("Generated", nrow(all_cluster_assignments), "cluster assignments for",
      length(unique(all_cluster_assignments$subid)), "participants\n")
}

# Insert final cluster results into gps_clusters table FIRST
if (nrow(all_participant_clusters) > 0) {
  # Add subject_id foreign key to cluster data
  clusters_for_db <- all_participant_clusters |>
    left_join(
      dbGetQuery(con, "SELECT id as subject_id, subid FROM subjects"),
      by = "subid"
    ) |>
    select(
      subject_id,
      lat, lon,
      n_points,
      first_visit,
      last_visit,
      total_visits,
      total_duration_hours
    )

  # Clear existing clusters and insert new results
  # Use DELETE instead of TRUNCATE to avoid foreign key constraint issues
  dbExecute(con, "DELETE FROM reverse_geocode_results")
  dbExecute(con, "DELETE FROM final_analysis WHERE cluster_id IS NOT NULL")
  dbExecute(con, "UPDATE raw_gps_points SET cluster_id = NULL WHERE cluster_id IS NOT NULL")
  dbExecute(con, "DELETE FROM gps_clusters")
  dbExecute(con, "ALTER SEQUENCE gps_clusters_cluster_id_seq RESTART WITH 1")
  dbWriteTable(con, "gps_clusters", clusters_for_db,
               append = TRUE, row.names = FALSE)

  cat("Inserted", nrow(clusters_for_db), "clusters into gps_clusters table\n")
}

# NOW update processed_gps_points with actual database cluster_ids
if (length(cluster_assignments) > 0) {

  # Get the actual cluster_ids that were assigned by the database
  db_clusters <- dbGetQuery(con, "
    SELECT cluster_id, subject_id, lat, lon
    FROM gps_clusters
  ")

  # Create mapping from participant + temp_cluster_id → actual cluster_id
  # by matching to the nearest cluster centroid in the database
  # Process by subject for efficiency
  assignments_with_ids <- all_cluster_assignments |>
    left_join(
      dbGetQuery(con, "SELECT id as subject_id, subid FROM subjects"),
      by = "subid"
    ) |>
    group_by(subject_id) |>
    group_modify(~ {
      subject_assignments <- .x
      subject_clusters <- db_clusters[db_clusters$subject_id == .y$subject_id, ]

      if (nrow(subject_clusters) == 0) {
        return(tibble(lat = numeric(0), lon = numeric(0),
                      time = as.POSIXct(character(0)), final_cluster_id = integer(0)))
      }

      # Vectorized distance calculation for all points to all clusters
      point_coords <- cbind(subject_assignments$lon, subject_assignments$lat)
      cluster_coords <- cbind(subject_clusters$lon, subject_clusters$lat)

      # Find nearest cluster for each point
      nearest_clusters <- apply(point_coords, 1, function(pt) {
        dists <- distHaversine(pt, cluster_coords)
        if (min(dists) <= radius_m * 2) {
          subject_clusters$cluster_id[which.min(dists)]
        } else {
          NA_integer_
        }
      })

      subject_assignments |>
        mutate(final_cluster_id = nearest_clusters) |>
        filter(!is.na(final_cluster_id))
    }) |>
    ungroup() |>
    select(subject_id, lat, lon, time, final_cluster_id)

  cat("Mapped", nrow(assignments_with_ids), "points to final cluster IDs\n")

  # Write to temporary table
  dbWriteTable(con, "temp_cluster_assignments", assignments_with_ids,
               overwrite = TRUE, temporary = TRUE)

  # Update using spatial matching with PostGIS (more reliable than timestamp matching)
  updates <- dbExecute(con, "
    WITH cluster_matches AS (
      SELECT DISTINCT ON (p.processed_id)
        p.processed_id,
        c.cluster_id,
        ST_Distance(
          p.geom::geography,
          c.geom::geography
        ) as distance_m
      FROM processed_gps_points p
      JOIN gps_clusters c ON p.subject_id = c.subject_id
      WHERE p.is_stationary = true
        AND p.cluster_id IS NULL
      ORDER BY p.processed_id, ST_Distance(p.geom::geography, c.geom::geography)
    )
    UPDATE processed_gps_points p
    SET cluster_id = cm.cluster_id
    FROM cluster_matches cm
    WHERE p.processed_id = cm.processed_id
      AND cm.distance_m <= 40  -- 2x the clustering radius (20m * 2)
  ")

  cat("Updated", updates, "rows in processed_gps_points with cluster assignments\n")
}
```

## Verification

Verify cluster assignments were successful:

```{r}
# Check cluster assignment coverage
assignment_stats <- dbGetQuery(con, "
  SELECT
    COUNT(*) as total_stationary_points,
    COUNT(cluster_id) as points_with_clusters,
    COUNT(DISTINCT cluster_id) as unique_clusters_assigned,
    ROUND(100.0 * COUNT(cluster_id) / NULLIF(COUNT(*), 0), 2) as pct_assigned
  FROM processed_gps_points
  WHERE is_stationary = true
")

print("Cluster Assignment Statistics:")
print(assignment_stats)

# Verify cluster distribution matches gps_clusters table
cluster_validation <- dbGetQuery(con, "
  SELECT
    (SELECT COUNT(*) FROM gps_clusters) as total_clusters_in_table,
    (SELECT COUNT(DISTINCT cluster_id) FROM processed_gps_points WHERE cluster_id IS NOT NULL) as clusters_used_in_points,
    (SELECT MIN(cluster_id) FROM processed_gps_points WHERE cluster_id IS NOT NULL) as min_cluster_id,
    (SELECT MAX(cluster_id) FROM processed_gps_points WHERE cluster_id IS NOT NULL) as max_cluster_id
")

print("Cluster Distribution Validation:")
print(cluster_validation)

# Per-subject cluster assignment summary
subject_stats <- dbGetQuery(con, "
  SELECT
    s.subid,
    COUNT(CASE WHEN p.is_stationary THEN 1 END) as stationary_points,
    COUNT(p.cluster_id) as points_with_clusters,
    COUNT(DISTINCT p.cluster_id) as unique_clusters
  FROM processed_gps_points p
  JOIN subjects s ON p.subject_id = s.id
  GROUP BY s.subid
  ORDER BY s.subid
  LIMIT 10
")

print("Sample of per-subject assignments:")
print(subject_stats)

# Sample daily cluster distribution for entropy analysis validation
daily_cluster_sample <- dbGetQuery(con, "
  SELECT
    s.subid,
    DATE(p.dttm_obs) as date,
    p.cluster_id,
    COUNT(*) as points_in_cluster
  FROM processed_gps_points p
  JOIN subjects s ON p.subject_id = s.id
  WHERE p.cluster_id IS NOT NULL
  GROUP BY s.subid, DATE(p.dttm_obs), p.cluster_id
  ORDER BY s.subid, date, points_in_cluster DESC
  LIMIT 20
")

print("Sample daily cluster distribution (for entropy calculation):")
print(daily_cluster_sample)
```

## Spatial Indexing

Add spatial indexes after clustering for efficient queries:

```{r}
# Create spatial indexes on populated data
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_processed_gps_geom ON processed_gps_points USING GIST (geom);")
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_gps_clusters_geom ON gps_clusters USING GIST (geom);")
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_processed_gps_cluster_id ON processed_gps_points (cluster_id);")
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_processed_gps_stationary ON processed_gps_points (is_stationary);")
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_processed_gps_subject ON processed_gps_points (subject_id);")
dbExecute(con, "CREATE INDEX IF NOT EXISTS idx_gps_clusters_subject ON gps_clusters (subject_id);")
```

Created spatial and performance indexes for efficient querying of processed GPS data and location clusters.

## Final Clustering Analysis

Generate descriptive statistics and summary of clustering results:

```{r}
# Calculate per-participant cluster counts
participant_cluster_counts <- all_participant_clusters |>
  group_by(subid) |>
  summarise(clusters_per_participant = n(), .groups = "drop")

participant_cluster_counts |>
  select(clusters_per_participant) |>
  skimr::skim()

all_participant_clusters |>
  select(
    gps_points_per_cluster = n_points,
    total_visits_per_cluster = total_visits, 
    duration_hours_per_cluster = total_duration_hours
  ) |>
  skimr::skim()
```

## Database Connection Cleanup

```{r}
# Close database connection
disconnect_gps_db(con)
```

## Next Steps

Based on the clustering results:

1. **If clustering successful**: Proceed to `04-reverse-geocoding.qmd`
2. **If parameter tuning needed**: Adjust clustering algorithms and re-run
3. **If performance issues**: Review spatial indexing strategy

### Using Cluster Data for Entropy Analysis

Each processed GPS point now has a `cluster_id` linking it to a specific location cluster. For Shannon's Entropy calculation of daily locations:

```r
# Example: Calculate daily location entropy per subject
daily_entropy <- dbGetQuery(con, "
  SELECT
    s.subid,
    DATE(p.dttm_obs) as date,
    p.cluster_id,
    COUNT(*) as points_per_cluster
  FROM processed_gps_points p
  JOIN subjects s ON p.subject_id = s.id
  WHERE p.cluster_id IS NOT NULL
  GROUP BY s.subid, DATE(p.dttm_obs), p.cluster_id
")

# Then calculate Shannon's Entropy:
# H = -Σ(p_i * log2(p_i))
# where p_i is the proportion of time spent in each location per day
```

### Troubleshooting Commands

```{bash eval=FALSE}
# Check cluster distribution
# docker-compose exec -T postgis psql -U postgres -d gps_analysis -c "SELECT COUNT(*) FROM gps_clusters;"

# Check raw GPS points with cluster assignments
# docker-compose exec -T postgis psql -U postgres -d gps_analysis -c "SELECT COUNT(*), cluster_id FROM raw_gps_points GROUP BY cluster_id;"

# View clustering performance stats
# docker-compose exec -T postgis psql -U postgres -d gps_analysis -c "SELECT subject_id, COUNT(*) as clusters FROM gps_clusters GROUP BY subject_id;"
```