---
title: "Location Entropy Analysis"
author: "Christopher Janssen"
date: "`r lubridate::today()`"
format:
  html:
    self-contained: true
    code-fold: false
    toc: true
    toc-depth: 3
editor_options:
  chunk_output_type: console
editor:
  markdown:
    wrap: 72
---

## Overview

This notebook calculates **daily** Shannon's entropy for location patterns to measure routine predictability in GPS mobility data. Higher entropy indicates more random/diverse movement patterns, while lower entropy indicates more predictable/routine behavior.

Each day receives its own entropy value based on the proportion of time spent at different locations that day, creating a time-varying predictor for relapse risk modeling.

## Setup

```{r setup, include=FALSE}

# Source setup script for database configuration
source(here::here("scripts/r/setup.R"))
source(here::here("scripts/r/database.R"))

# ggplot
theme_set(theme_bw())

# Database connection
con <- connect_gps_db()
```

## Location Entropy Calculation

```{r entropy-functions}
# Calculate Shannon entropy for location patterns
calculate_location_entropy <- function(cluster_durations) {
  # Remove any NA or zero durations
  cluster_durations <- cluster_durations[!is.na(cluster_durations) & cluster_durations > 0]

  if (length(cluster_durations) == 0) return(NA)
  if (length(cluster_durations) == 1) return(0)  # Perfect predictability

  # Calculate probabilities (proportion of time in each cluster)
  total_time <- sum(cluster_durations)
  probabilities <- cluster_durations / total_time

  # Calculate Shannon entropy: H = -Σ(p_i * log2(p_i))
  entropy <- -sum(probabilities * log2(probabilities))

  return(entropy)
}

# Calculate predictability from entropy (vectorized)
calculate_predictability <- function(entropy, n_unique_locations) {
  # Handle vectorized inputs
  result <- ifelse(
    is.na(entropy) | n_unique_locations <= 1,
    NA,
    {
      max_entropy <- log2(n_unique_locations)
      (max_entropy - entropy) / max_entropy
    }
  )
  return(result)
}
```

## Methodology: How Cluster Assignment Enables Daily Entropy

Each GPS point in `processed_gps_points` now has a `cluster_id` linking it to a specific location cluster from `gps_clusters`. This enables us to:

1. **Track daily location patterns**: Group points by subject + date + cluster_id
2. **Calculate time proportions**: Sum durations within each cluster per day
3. **Compute Shannon entropy**: H = -Σ(p_i × log₂(p_i)) where p_i is the proportion of time in location i

**Example**: If someone spends 60% of their day at home (cluster 1), 30% at work (cluster 2), and 10% at a coffee shop (cluster 3):
- Entropy = -(0.6×log₂(0.6) + 0.3×log₂(0.3) + 0.1×log₂(0.1)) ≈ 1.30 bits

Lower entropy = more predictable routine (e.g., 90% at one location)
Higher entropy = less predictable routine (e.g., time spread evenly across many locations)

## Data Preparation

```{r data-prep}
# Get daily cluster visits with durations
# Join with subjects table to get subid for export compatibility
daily_cluster_data <- dbGetQuery(con, "
  SELECT
    s.subid,
    p.subject_id,
    p.cluster_id,
    DATE(p.dttm_obs) as date,
    COUNT(*) as n_points,
    -- Calculate total duration by summing time intervals between consecutive points
    -- duration_mins represents time SINCE previous point, so sum gives total time in cluster
    SUM(p.duration_mins) / 60.0 as duration_hours
  FROM processed_gps_points p
  JOIN subjects s ON p.subject_id = s.id
  WHERE p.cluster_id IS NOT NULL
    AND p.is_stationary = true  -- Only stationary points have valid cluster assignments
  GROUP BY s.subid, p.subject_id, p.cluster_id, DATE(p.dttm_obs)
  ORDER BY s.subid, date, p.cluster_id
")

# Data quality summary
cat("Daily cluster data summary:\n")
cat("  Total cluster-day observations:", nrow(daily_cluster_data), "\n")
cat("  Unique subjects:", length(unique(daily_cluster_data$subid)), "\n")
cat("  Unique days:", length(unique(daily_cluster_data$date)), "\n")
cat("  Date range:", min(daily_cluster_data$date), "to", max(daily_cluster_data$date), "\n")

# Check for any invalid durations
invalid_durations <- daily_cluster_data |>
  filter(is.na(duration_hours) | duration_hours < 0)

if (nrow(invalid_durations) > 0) {
  cat("  WARNING:", nrow(invalid_durations), "observations with invalid durations\n")
} else {
  cat("  ✓ All durations valid\n")
}
```

## Validation: Example Daily Pattern

```{r validation-example}
# Show example calculation for one subject-day to validate methodology
example_day <- daily_cluster_data |>
  group_by(subid, date) |>
  filter(n() >= 3) |>  # Days with 3+ locations for interesting entropy
  ungroup() |>
  slice(1:10) |>  # Take first 10 rows (one subject-day with multiple clusters)
  group_by(subid, date) |>
  slice(1:n()) |>
  ungroup()

if (nrow(example_day) > 0) {
  cat("\nExample: Daily location pattern for validation\n")
  cat("Subject:", example_day$subid[1], "| Date:", as.character(example_day$date[1]), "\n\n")

  example_summary <- example_day |>
    mutate(
      proportion = duration_hours / sum(duration_hours),
      log2_p = log2(proportion),
      entropy_contribution = -proportion * log2_p
    ) |>
    select(cluster_id, duration_hours, proportion, entropy_contribution)

  print(example_summary)

  cat("\nTotal entropy for this day:", sum(example_summary$entropy_contribution), "bits\n")
  cat("Number of unique locations:", nrow(example_summary), "\n")
}
```

## Calculate Daily Location Entropy

```{r calculate-entropy}
# Calculate entropy for each subject-day
daily_entropy <- daily_cluster_data |>
  group_by(subid, date) |>
  summarise(
    subject_id = first(subject_id),  # Keep for database foreign key
    unique_locations = n_distinct(cluster_id),
    total_time_hours = sum(duration_hours, na.rm = TRUE),
    total_points = sum(n_points, na.rm = TRUE),
    # Calculate Shannon entropy based on time proportions
    location_entropy = calculate_location_entropy(duration_hours),
    .groups = "drop"
  ) |>
  mutate(
    # Normalized predictability score (0 = random, 1 = perfectly predictable)
    location_predictability = calculate_predictability(location_entropy, unique_locations)
  ) |>
  # Filter days with sufficient data quality
  # Require at least 2 locations (entropy undefined for 1 location)
  # Require at least 1 hour of tracked time for meaningful daily measure
  filter(
    unique_locations >= 2,
    total_time_hours >= 1,
    !is.na(location_entropy)
  )

# Summary of daily entropy calculation
cat("\nDaily Location Entropy Calculation Summary:\n")
cat("  Total person-days with valid entropy:", nrow(daily_entropy), "\n")
cat("  Subjects with entropy data:", length(unique(daily_entropy$subid)), "\n")
cat("  Mean daily entropy:", round(mean(daily_entropy$location_entropy, na.rm = TRUE), 3), "bits\n")
cat("  Mean daily predictability:", round(mean(daily_entropy$location_predictability, na.rm = TRUE), 3), "\n")
cat("  Mean locations per day:", round(mean(daily_entropy$unique_locations, na.rm = TRUE), 1), "\n")
```

## Summary Statistics

```{r summary-stats}
# Overall daily entropy statistics
daily_entropy |>
  summarise(
    n_person_days = n(),
    n_subjects = n_distinct(subject_id),
    mean_entropy = mean(location_entropy, na.rm = TRUE),
    sd_entropy = sd(location_entropy, na.rm = TRUE),
    median_entropy = median(location_entropy, na.rm = TRUE),
    min_entropy = min(location_entropy, na.rm = TRUE),
    max_entropy = max(location_entropy, na.rm = TRUE),
    mean_predictability = mean(location_predictability, na.rm = TRUE),
    mean_unique_locations = mean(unique_locations, na.rm = TRUE)
  ) |>
  kableExtra::kable()

# Per-subject statistics
subject_summary <- daily_entropy |>
  group_by(subid) |>
  summarise(
    n_days = n(),
    mean_daily_entropy = mean(location_entropy, na.rm = TRUE),
    sd_daily_entropy = sd(location_entropy, na.rm = TRUE),
    .groups = "drop"
  )

subject_summary |>
  summarise(
    n_subjects = n(),
    mean_days_per_subject = mean(n_days),
    mean_within_person_entropy = mean(mean_daily_entropy),
    mean_within_person_sd = mean(sd_daily_entropy, na.rm = TRUE)
  ) |>
  kableExtra::kable()
```

## Visualizations

```{r entropy-distribution}
# Distribution of daily location entropy
ggplot(daily_entropy, aes(x = location_entropy)) +
  geom_histogram(bins = 30, fill = "#2196F3", alpha = 0.7, color = "white") +
  geom_vline(aes(xintercept = mean(location_entropy, na.rm = TRUE)),
             color = "#FF5722", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Distribution of Daily Location Entropy",
    subtitle = paste("n =", format(nrow(daily_entropy), big.mark=","), "person-days"),
    x = "Daily Location Entropy (bits)",
    y = "Number of Days"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60")
  )
```

```{r entropy-vs-locations}
# Relationship between entropy and number of unique locations visited per day
ggplot(daily_entropy, aes(x = unique_locations, y = location_entropy)) +
  geom_point(alpha = 0.3, color = "#2196F3", size = 1.5) +
  geom_smooth(method = "lm", color = "#FF5722", se = TRUE) +
  labs(
    title = "Daily Entropy vs Number of Locations Visited",
    x = "Number of Unique Locations (per day)",
    y = "Daily Location Entropy (bits)"
  )
```

```{r within-person-variation}
# Show within-person variation for a sample of subjects
sample_subjects <- daily_entropy |>
  group_by(subid) |>
  filter(n() >= 30) |>  # At least 30 days of data
  ungroup() |>
  distinct(subid) |>
  slice_sample(n = 6) |>
  pull(subid)

daily_entropy |>
  filter(subid %in% sample_subjects) |>
  ggplot(aes(x = date, y = location_entropy)) +
  geom_line(color = "#2196F3", alpha = 0.6) +
  geom_point(color = "#2196F3", size = 1, alpha = 0.4) +
  facet_wrap(~subid, ncol = 2, scales = "free_x") +
  labs(
    title = "Daily Entropy Over Time (Sample of 6 Subjects)",
    x = "Date",
    y = "Daily Location Entropy (bits)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Export Results for Analysis

```{r export-data}
# Create final dataset for analysis (daily level)
# Export version with subid for research drive compatibility
export_data <- daily_entropy |>
  select(
    subid,
    date,
    location_entropy,
    location_predictability,
    unique_locations,
    total_time_hours,
    total_points
  ) |>
  arrange(subid, date)

# Database version with subject_id for foreign key
database_data <- daily_entropy |>
  select(
    subject_id,
    date,
    location_entropy,
    location_predictability,
    unique_locations,
    total_time_hours
  ) |>
  arrange(subject_id, date)

# Preview export data
cat("\nPreview of export data (first 20 rows):\n")
head(export_data, 20)

# Export to CSV on research drive
output_file <- file.path(path_processed, "daily_location_entropy.csv")
readr::write_csv(export_data, output_file)
cat("\n✓ Exported daily entropy to:", output_file, "\n")
cat("  Rows:", nrow(export_data), "\n")
cat("  Subjects:", length(unique(export_data$subid)), "\n")
```

## Store Results in Database

```{r store-results}
# Clear existing entropy results before inserting new data
dbExecute(con, "TRUNCATE TABLE daily_location_entropy")

# Insert results using database_data (has subject_id for foreign key)
dbWriteTable(con, "daily_location_entropy", database_data,
             append = TRUE, row.names = FALSE)

# Verify insertion
verification <- dbGetQuery(con, "
  SELECT
    COUNT(*) as n_rows,
    COUNT(DISTINCT subject_id) as n_subjects,
    MIN(date) as earliest_date,
    MAX(date) as latest_date,
    ROUND(AVG(location_entropy)::numeric, 3) as mean_entropy,
    ROUND(AVG(location_predictability)::numeric, 3) as mean_predictability
  FROM daily_location_entropy
")

cat("\nStored daily location entropy results:\n")
print(verification)
```

```{r cleanup}
disconnect_gps_db(con)
```

## Summary

This analysis calculated **daily** location entropy for **`r format(nrow(export_data), big.mark=",")`** person-days across **`r length(unique(export_data$subid))`** subjects.

Daily entropy values range from **`r round(min(export_data$location_entropy, na.rm=TRUE), 3)`** to **`r round(max(export_data$location_entropy, na.rm=TRUE), 3)`** bits, with a mean of **`r round(mean(export_data$location_entropy, na.rm=TRUE), 3)`** bits.

